# Troubleshooting import and export failures<a name="failed-tasks"></a>

You can [turn on logging](cw-event-logging.md) to CloudWatch Logs to log information about any failures experienced while importing or exporting files using data repository tasks\. For information about CloudWatch Logs event logs, see [Data repository event logs](data-repo-event-logs.md)\.

When a data repository task fails, you can find the number of files that Amazon FSx failed to process in **Files failed to export** on the console's **Task status **page\. Or you can use the CLI or API and view the task's `Status: FailedCount` property\. For information about accessing this information, see [Accessing data repository tasks](managing-data-repo-task.md#view-data-repo-tasks)\. 

For data repository tasks, Amazon FSx also optionally provides information about the specific files and directories that failed in a completion report\. The task completion report contains the file or directory path on the Lustre file system that failed, its status, and the failure reason\. For more information, see [Working with task completion reports](task-completion-report.md)\.

A data repository task can fail for several reasons, including those listed following\.


| Error Code | Explanation | 
| --- | --- | 
|  `PathSizeTooLong`  |  The export path is too long\. The maximum object key length supported by S3 is 1,024 characters\.  | 
|  `FileSizeTooLarge`  |  The maximum object size supported by Amazon S3 is 5 TiB\.  | 
|  `S3AccessDenied`  |  Access was denied to Amazon S3 for a data repository export or import task\. For export tasks, the Amazon FSx file system must have permission to perform the `S3:PutObject` operation to export to a linked data repository on S3\. This permission is granted in the `AWSServiceRoleForFSxS3Access_fs-0123456789abcdef0` service\-linked role\. For more information, see [Using service\-linked roles for Amazon FSx for Lustre](using-service-linked-roles.md)\. For export tasks, because the export task requires data to flow outside a file system's VPC, this error can occur if the target repository has a bucket policy that contains one of the `aws:SourceVpc` or `aws:SourceVpce` IAM global condition keys\. For import tasks, the Amazon FSx file system must have permission to perform the `S3:HeadObject` and `S3:GetObject` operations to import from a linked data repository on S3\. For import tasks, if your S3 bucket uses server\-side encryption with customer managed keys stored in AWS Key Management Service \(SSE\-KMS\), you must follow the policy configurations in [Working with server\-side encrypted Amazon S3 buckets](create-dra-linked-data-repo.md#s3-server-side-encryption-support)\. If your S3 bucket contains objects uploaded from a different AWS account than your file system linked S3 bucket account, you can ensure that your data repository tasks can modify S3 metadata or overwrite S3 objects regardless of which account uploaded them\. We recommend that you enable the S3 Object Ownership feature for your S3 bucket\. This feature enables you to take ownership of new objects that other AWS accounts upload to your bucket, by forcing uploads to provide the `-/-acl bucket-owner-full-control` canned ACL\. You enable S3 Object Ownership by choosing the **Bucket owner preferred** option in your S3 bucket\. For more information, see [Controlling ownership of uploaded objects using S3 Object Ownership](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html) in the *Amazon S3 User Guide*\.  | 
|  `S3Error`  |  Amazon FSx encountered an S3\-related error that wasn't `S3AccessDenied`\.  | 
|  `S3ObjectPathNotPosixCompliant`  |  The Amazon S3 object exists but can't be imported because it isn't a POSIX\-compliant object\. For information about supported POSIX metadata, see [POSIX metadata support for data repositories](overview-dra-data-repo.md#posix-metadata-support)\.  | 
|  `S3FileDeleted`  | Amazon FSx was unable to export a hard link file because the source file doesn't exist in the data repository\. | 
|  `S3ObjectNotFound`  | Amazon FSx was unable to import or export the file because it doesn't exist in the data repository\. | 
|  `S3ObjectInUnsupportedTier`  | Amazon FSx successfully imported a non\-symlink object from an S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage class\. The `FileStatus` will be `succeeded with warning` in the task completion report\. The warning indicates that to retrieve the data, you must restore the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive object first and then use an `hsm_restore` command to import the object\.  | 
|  `S3SymlinkInUnsupportedTier`  | Amazon FSx was unable to import a symlink object because it's in an Amazon S3 storage class that is not supported, such as an S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage class\. The `FileStatus` will be `failed` in the task completion report\. | 
|  `ResourceBusy`  | Amazon FSx was unable to export the file because it was being modified by another client on the file system\. You can retry the DataRepositoryTask after your workflow has finished writing to the file\. | 
|  `InternalError`  |  An error occurred within the Amazon FSx file system\. Generally, this error code means that The Amazon FSx file system that the failed task ran on is in a FAILED lifecycle state\. When this occurs, the affected files might not be recoverable due to data loss\. Otherwise, you can use hierarchical storage management \(HSM\) commands to export the files and directories to the data repository on S3\. For more information, see [Exporting files using HSM commands](exporting-files-hsm.md)\.  | 